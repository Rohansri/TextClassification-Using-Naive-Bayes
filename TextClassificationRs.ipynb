{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as ma\n",
    "import itertools                                            #this is to slice the dictionary to get only max frequecvy values\n",
    "from sklearn import datasets\n",
    "from nltk.corpus import stopwords                           #to get list of stopwords\n",
    "from sklearn import model_selection\n",
    "from nltk.tokenize import word_tokenize                     #used in removing stopwords from data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary,x,clas):                            # it returns the actual probability of input x over class clas\n",
    "    \n",
    "    count=ma.log(dictionary[clas][\"count\"])-ma.log(dictionary[\"total\"])                      # it is probability of(y=class(clas))\n",
    "    features_number=len(dictionary[clas].keys())-2                                           #total number of features\n",
    "    for j in range(features_number):          # calculting the probabilty over each feature the later we will take log() sum of all            \n",
    "        if(x[j]==0):                           #if input x have zero frequency over the feature so its probabiluty will not counted\n",
    "            continue\n",
    "        count_xj_in_feature_j=dictionary[clas][j]+1                        # it is the total frequency of feature j in class->clas\n",
    "        count_clas_ele_in_feature=dictionary[clas][\"Grand_total\"]+ features_number     # it is total number of words in class->clas\n",
    "        p=ma.log(count_xj_in_feature_j)-ma.log(count_clas_ele_in_feature)          #summing all small probabilities of all features\n",
    "        count=count+p\n",
    "    return count                                                                   # returning the probabilty\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlecol(dictionary,x):        #singlecol gives the prediction(output) of single colum at a time\n",
    "    \n",
    "    best_prob=-1000                # giving any value to initialise best_prob\n",
    "    best_cls=-1                    # giving any value to initialise best_cls\n",
    "    classes=dictionary.keys()      #dictionary .keys have all the classes names\n",
    "    val=True\n",
    "    for clas in classes:           # checking probabily on one class at a time \n",
    "        if clas==\"total\":          # total is not a class so ignore it\n",
    "            continue\n",
    "        clas_p=probability(dictionary,x,clas)        # clas_p will have probability of input x for class clas\n",
    "        if(val or clas_p>best_prob):\n",
    "            best_prob=clas_p\n",
    "            best_cls=clas\n",
    "        val=False\n",
    "    return best_cls                 #returns the best_cls with maximum probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(dictionary,xx_test):         # to obtain output list\n",
    "    \n",
    "    lst=[]\n",
    "    for x in xx_test:                    #going through test_data row wise\n",
    "        pred=singlecol(dictionary,x)     # as we get a answer by one column we are appending it to list\n",
    "        lst.append(pred)\n",
    "    return lst\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(xx_train,y_train,features):              # this function is to train algorithm over training data\n",
    "    result={}                                    #we will use dictionary and create nested dictionary where needed\n",
    "    classes=set(y_train)\n",
    "    \n",
    "    for current_class in classes:                # acceccing all classes one by one \n",
    "        result[current_class]={}\n",
    "        result[\"total\"]=len(xx_train)            # it will hold length of entire xx_train set\n",
    "        current_class_rows=(y_train==current_class)            #fetching only current_class colums\n",
    "        x_train_current=xx_train[current_class_rows]           #spliting x_train for only current_class\n",
    "        y_train_current=y_train[current_class_rows]            #spliting y_train for only current_class\n",
    "        result[current_class][\"count\"]=len(x_train_current)   # it will hold count of current_class (it will be used at time of calculating probability)\n",
    "        features_total=xx_train.shape[1]                      #feature size is nothin but the columns of xx_train\n",
    "        a=0\n",
    "        for j in range(len(features)):\n",
    "            result[current_class][j]=(x_train_current[:,j].sum())         #it will hold frequency of feature j\n",
    "            a+=result[current_class][j]\n",
    "        result[current_class][\"Grand_total\"]=a                            #it will hold count of entire words in current_class\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85        52\n",
      "           1       0.52      0.76      0.62        68\n",
      "           2       1.00      0.02      0.03        60\n",
      "           3       0.47      0.67      0.55        58\n",
      "           4       0.55      0.85      0.67        62\n",
      "           5       1.00      0.43      0.60        56\n",
      "           6       0.71      0.86      0.78        57\n",
      "           7       0.73      0.84      0.78        51\n",
      "           8       0.85      0.87      0.86        63\n",
      "           9       0.81      0.88      0.85        68\n",
      "          10       0.98      0.79      0.87        56\n",
      "          11       0.97      0.93      0.95        68\n",
      "          12       0.64      0.63      0.64        71\n",
      "          13       0.90      0.73      0.80        59\n",
      "          14       0.79      0.87      0.83        47\n",
      "          15       0.86      0.88      0.87        75\n",
      "          16       0.92      0.86      0.89        42\n",
      "          17       0.95      0.75      0.84        48\n",
      "          18       0.77      0.85      0.80        39\n",
      "          19       0.65      0.62      0.63        32\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1132\n",
      "   macro avg       0.79      0.75      0.74      1132\n",
      "weighted avg       0.79      0.75      0.73      1132\n",
      "\n",
      "[[44  0  0  0  1  0  0  0  0  0  0  0  0  0  0  1  1  0  0  5]\n",
      " [ 0 52  0  3  6  0  2  0  0  0  0  2  2  1  0  0  0  0  0  0]\n",
      " [ 0 12  1 30  8  0  6  1  0  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  5  0 39  7  0  3  0  0  0  0  0  4  0  0  0  0  0  0  0]\n",
      " [ 0  3  0  2 53  0  2  0  0  0  0  0  1  1  0  0  0  0  0  0]\n",
      " [ 1 18  0  2  5 24  1  0  0  0  0  0  2  1  2  0  0  0  0  0]\n",
      " [ 1  1  0  1  4  0 49  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 43  4  0  0  0  2  0  2  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  3 55  1  0  0  1  0  1  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  1  2  3 60  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  1  0 10 44  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  1  0  0  1  0 63  1  0  0  0  0  0  1  0]\n",
      " [ 0  3  0  5 11  0  1  4  0  0  0  0 45  0  0  0  1  0  0  1]\n",
      " [ 1  1  0  0  0  0  0  2  1  0  0  0  6 43  2  3  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1  1  0  0  0  0  0 41  0  0  0  2  1]\n",
      " [ 1  2  0  1  0  0  0  0  0  0  0  0  1  2  2 66  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0 36  0  3  1]\n",
      " [ 3  0  0  0  1  0  1  0  0  1  0  0  0  0  1  1  0 36  3  1]\n",
      " [ 0  0  0  0  0  0  1  0  1  0  0  0  0  0  0  1  0  1 33  2]\n",
      " [ 1  0  0  0  0  0  0  1  0  1  1  0  0  0  1  5  1  1  0 20]]\n",
      "\n",
      "\n",
      "---------COMPARISION---------------this classification is due to sklearn library------------\n",
      "0.7173144876325088\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84        52\n",
      "           1       0.56      0.71      0.63        68\n",
      "           2       0.50      0.07      0.12        60\n",
      "           3       0.44      0.66      0.52        58\n",
      "           4       0.57      0.79      0.66        62\n",
      "           5       0.62      0.50      0.55        56\n",
      "           6       0.63      0.82      0.71        57\n",
      "           7       0.62      0.73      0.67        51\n",
      "           8       0.65      0.87      0.74        63\n",
      "           9       0.81      0.82      0.82        68\n",
      "          10       0.96      0.77      0.85        56\n",
      "          11       0.98      0.88      0.93        68\n",
      "          12       0.69      0.63      0.66        71\n",
      "          13       0.84      0.69      0.76        59\n",
      "          14       0.81      0.81      0.81        47\n",
      "          15       0.88      0.80      0.84        75\n",
      "          16       0.84      0.76      0.80        42\n",
      "          17       0.97      0.71      0.82        48\n",
      "          18       0.86      0.77      0.81        39\n",
      "          19       0.66      0.66      0.66        32\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      1132\n",
      "   macro avg       0.73      0.72      0.71      1132\n",
      "weighted avg       0.73      0.72      0.71      1132\n",
      "\n",
      "[[46  1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  4]\n",
      " [ 0 48  0  4  4  4  4  0  1  0  0  1  2  0  0  0  0  0  0  0]\n",
      " [ 0 11  4 29  5  3  3  3  1  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  3  0 38  9  1  3  1  0  0  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  5 49  1  2  1  0  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0 11  0  3  4 28  3  2  1  0  0  0  1  1  1  0  0  0  0  1]\n",
      " [ 0  0  2  2  3  1 47  1  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0 37  9  0  0  0  1  0  2  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  2  2 55  1  0  0  1  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  1  2  3 56  2  0  2  0  0  0  1  0  0  0]\n",
      " [ 0  0  1  0  0  0  1  2  0  9 43  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  1  1  1  1  0 60  1  0  0  0  1  0  1  0]\n",
      " [ 1  3  0  5 10  1  2  2  1  0  0  0 45  0  0  0  0  0  0  1]\n",
      " [ 1  0  0  0  0  1  1  3  2  0  0  0  4 41  2  2  1  0  0  1]\n",
      " [ 0  1  0  0  0  1  0  2  1  1  0  0  0  2 38  0  0  0  0  1]\n",
      " [ 3  2  0  1  0  0  0  0  4  0  0  0  1  3  1 60  0  0  0  0]\n",
      " [ 0  0  0  0  1  2  1  0  2  0  0  0  0  0  1  0 32  0  2  1]\n",
      " [ 3  1  0  0  0  0  2  0  1  1  0  0  1  1  0  1  1 34  2  0]\n",
      " [ 2  0  0  0  0  0  1  0  2  0  0  0  0  0  0  0  1  1 30  2]\n",
      " [ 1  0  0  0  1  0  1  1  1  0  0  0  0  0  1  4  1  0  0 21]]\n"
     ]
    }
   ],
   "source": [
    "news=datasets.fetch_20newsgroups()                              # loading data from datasets   to a news name dataframe\n",
    "x=news.data\n",
    "y=news.target\n",
    "\n",
    "x_train,x_test,y_train,y_test=model_selection.train_test_split(x,y,test_size=0.1,random_state=3)         #doing spliting for train and test data\n",
    "\n",
    "len_data=len(x_train)\n",
    "dictionary=dict()                           #in this dictionary we will store frequency of each word from entire dataset by removing stop_words\n",
    "for j in range(len_data):\n",
    "    data=x_train[j]\n",
    "    word_tokens = word_tokenize(data)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    for word in filtered_sentence:\n",
    "        if word in dictionary:\n",
    "            dictionary[word]+=1\n",
    "        else:\n",
    "            dictionary[word]=1\n",
    "new_dict={}                                               #this is the reverse sorted form of dictionary used above\n",
    "for key,value in sorted(dictionary.items(),key=lambda item: item[1],reverse=True):\n",
    "    new_dict[key]=value\n",
    "a=dict(itertools.islice(new_dict.items(),3000))           # slicing over bigger ditionary to get max  frequency 3000 data only\n",
    "features=[]                                               # features is the list of keys of dictionary (a) \n",
    "for i in a.keys():\n",
    "    features.append(i)\n",
    "    \n",
    "xx_train=np.zeros((len(x_train),len(features)))           #modifing x_train to xx_train which is 2d and have frequency of each word of features \n",
    "for i in range(len(x_train)):\n",
    "    data=x_train[i]\n",
    "    \n",
    "    word_tokens = word_tokenize(data)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    for j in filtered_sentence:\n",
    "        if j in features:\n",
    "            xx_train[i][features.index(j)]+=1\n",
    "            \n",
    "xx_test=np.zeros((len(x_test),len(features)))             #xx_test is modified from x_test which is 2d now and have frequency format of each feature\n",
    "for i in range(len(x_test)):\n",
    "    data2=x_test[i]\n",
    "    word_tok=word_tokenize(data2)\n",
    "    \n",
    "    fil_sentence = [w for w in word_tok if not w in stop_words]\n",
    "    \n",
    "    for j in fil_sentence:\n",
    "        if j in features:\n",
    "            xx_test[i][features.index(j)]+=1\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "final_dict=fit(xx_train,y_train,features)        #calling fit function over data xx_train and y_train  \n",
    "y_pred=predict(final_dict,xx_test)               # predict will return the output list of classes  (output)\n",
    "    \n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix      #importing these to check correctness of y_pred(output)\n",
    "print(classification_report(y_test,y_pred))                             #it will give precission and recoil data\n",
    "print(confusion_matrix(y_test,y_pred))                      # it will print confusion matrix to show how is the output result\n",
    "       \n",
    "print()\n",
    "print()\n",
    "print('---------COMPARISION---------------this classification is due to sklearn library------------')\n",
    "from sklearn.naive_bayes import MultinomialNB          # now doing the same fit and predict by MultinomialNB library function\n",
    "arg1=MultinomialNB()\n",
    "arg1.fit(xx_train,y_train)\n",
    "y_pred2=arg1.predict(xx_test)\n",
    "print(arg1.score(xx_test,y_test))   # getting a score of 0.72\n",
    "\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(confusion_matrix(y_test,y_pred2))        \n",
    "\n",
    "    \n",
    "\n",
    "# the average of precision using sketch is 0.77 and due to library it is 0.74 which are nearly same so our code is giving the correct output\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
